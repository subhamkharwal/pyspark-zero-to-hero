{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ammfat/pyspark-zero-to-hero/blob/colab/08_reading_from_csv_files.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "9913130b-9ed3-4a87-a01d-a13cae0e3204",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "9913130b-9ed3-4a87-a01d-a13cae0e3204",
        "outputId": "84122222-5a36-43fc-bc7a-879df649019f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f9d6bc2b2d0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://50949f410618:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Reading from CSV Files</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# Spark Session\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (\n",
        "    SparkSession\n",
        "    .builder\n",
        "    .appName(\"Reading from CSV Files\")\n",
        "    .master(\"local[*]\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from os import path, makedirs\n",
        "\n",
        "if not path.exists(\"data\"):\n",
        "    makedirs(\"data\")\n",
        "\n",
        "if not path.exists(\"data/input\"):\n",
        "    makedirs(\"data/input\")\n",
        "\n",
        "data=\"\"\"employee_id,department_id,name,age,gender,salary,hire_date\n",
        "001,101,John Doe,30,Male,50000,2015-01-01\n",
        "002,101,Jane Smith,25,Female,45000,2016-02-15\n",
        "003,102,Bob Brown,35,Male,55000,2014-05-01\n",
        "004,102,Alice Lee,28,Female,48000,2017-09-30\n",
        "005,103,Jack Chan,40,Male,60000,2013-04-01\n",
        "006,103,Jill Wong,32,Female,52000,2018-07-01\n",
        "007,101,James Johnson,42,Male,70000,2012-03-15\n",
        "008,102,Kate Kim,29,Female,51000,2019-10-01\n",
        "009,103,Tom Tan,33,Male,58000,2016-06-01\n",
        "010,104,Lisa Lee,27,Female,47000,2018-08-01\n",
        "011,104,David Park,38,Male,65000,2015-11-01\n",
        "012,105,Susan Chen,31,Female,54000,2017-02-15\n",
        "013,106,Brian Kim,45,Male,75000,2011-07-01\n",
        "014,107,Emily Lee,26,Female,46000,2019-01-01\n",
        "015,106,Michael Lee,37,Male,63000,2014-09-30\n",
        "016,107,Kelly Zhang,30,Female,49000,2018-04-01\n",
        "017,105,George Wang,34,Male,57000,2016-03-15\n",
        "018,104,Nancy Liu,29,Female,50000,2017-06-01\n",
        "019,103,Steven Chen,36,Male,62000,2015-08-01\n",
        "020,102,Grace Kim,32,Female,53000,2018-11-01\n",
        "\"\"\"\n",
        "\n",
        "data_new=\"\"\"employee_id,department_id,name,age,gender,salary,hire_date\n",
        "001,101,John Doe,30,Male,50000,2015-01-01\n",
        "002,101,Jane Smith,25,Female,45000,2016-02-15\n",
        "003,102,Bob Brown,35,Male,55000,2014-05-01\n",
        "004,102,Alice Lee,28,Female,48000,2017-09-30\n",
        "005,103,Jack Chan,40,Male,60000,2013-04-01\n",
        "006,103,Jill Wong,32,Female,52000,2018-07-01\n",
        "007,101,James Johnson,42,Male,Low,2012-03-15\n",
        "008,102,Kate Kim,29,Female,51000,2019-10-01\n",
        "009,103,Tom Tan,33,Male,58000,2016-06-01\n",
        "010,104,Lisa Lee,27,Female,47000,2018-08-01\n",
        "011,104,David Park,38,Male,65000,no date\n",
        "012,105,Susan Chen,31,Female,54000,2017-02-15\n",
        "013,106,Brian Kim,45,Male,75000,2011-07-01\n",
        "014,107,Emily Lee,26,Female,46000,2019-01-01\n",
        "015,106,Michael Lee,37,Male,63000,2014-09-30\n",
        "016,107,Kelly Zhang,30,Female,49000,2018-04-01\n",
        "017,105,George Wang,34,Male,57000,2016-03-15\n",
        "018,104,Nancy Liu,29,Female,50000,2017-06-01\n",
        "019,103,Steven Chen,36,Male,62000,2015-08-01\n",
        "020,102,Grace Kim,32,Female,53000,2018-11-01\n",
        "\"\"\"\n",
        "\n",
        "with open(\"data/input/emp.csv\", \"w\") as f:\n",
        "    f.write(data)\n",
        "\n",
        "\n",
        "with open(\"data/input/emp_new.csv\", \"w\") as f:\n",
        "    f.write(data_new)"
      ],
      "metadata": {
        "id": "dScYrNJeH25n"
      },
      "id": "dScYrNJeH25n",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "92dec59b-66eb-4367-bac0-8b4332150f12",
      "metadata": {
        "id": "92dec59b-66eb-4367-bac0-8b4332150f12"
      },
      "outputs": [],
      "source": [
        "# Read a csv file into dataframe\n",
        "\n",
        "df = spark.read.format(\"csv\").option(\"header\", True).option(\"inferSchema\", True).load(\"data/input/emp.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "5112f6bc-bafa-4104-87f8-295fc6960c2d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5112f6bc-bafa-4104-87f8-295fc6960c2d",
        "outputId": "7d589ad3-7599-4f1c-bb02-41da59d9eaf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- employee_id: integer (nullable = true)\n",
            " |-- department_id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            " |-- hire_date: date (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "01f4e082-acb3-4d82-8369-e001ab5d0120",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01f4e082-acb3-4d82-8369-e001ab5d0120",
        "outputId": "9fc7992b-fec9-4e06-cbd8-6b4ffd484e4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-------------+---+------+------+----------+\n",
            "|employee_id|department_id|         name|age|gender|salary| hire_date|\n",
            "+-----------+-------------+-------------+---+------+------+----------+\n",
            "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01|\n",
            "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15|\n",
            "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|\n",
            "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30|\n",
            "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|\n",
            "|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01|\n",
            "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15|\n",
            "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01|\n",
            "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|\n",
            "|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n",
            "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01|\n",
            "|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15|\n",
            "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|\n",
            "|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01|\n",
            "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|\n",
            "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|\n",
            "|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15|\n",
            "|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|\n",
            "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|\n",
            "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01|\n",
            "+-----------+-------------+-------------+---+------+------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "a18b8672-06f9-44da-abda-af9df58f7509",
      "metadata": {
        "id": "a18b8672-06f9-44da-abda-af9df58f7509"
      },
      "outputs": [],
      "source": [
        "# Reading with Schema\n",
        "_schema = \"employee_id int, department_id int, name string, age int, gender string, salary double, hire_date date\"\n",
        "\n",
        "df_schema = spark.read.format(\"csv\").option(\"header\",True).schema(_schema).load(\"data/input/emp.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "7d76791f-8d4b-4afb-b12d-0cd136317667",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d76791f-8d4b-4afb-b12d-0cd136317667",
        "outputId": "29a17898-7ef8-42c2-9db9-a43ca32be78f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-------------+---+------+-------+----------+\n",
            "|employee_id|department_id|         name|age|gender| salary| hire_date|\n",
            "+-----------+-------------+-------------+---+------+-------+----------+\n",
            "|          1|          101|     John Doe| 30|  Male|50000.0|2015-01-01|\n",
            "|          2|          101|   Jane Smith| 25|Female|45000.0|2016-02-15|\n",
            "|          3|          102|    Bob Brown| 35|  Male|55000.0|2014-05-01|\n",
            "|          4|          102|    Alice Lee| 28|Female|48000.0|2017-09-30|\n",
            "|          5|          103|    Jack Chan| 40|  Male|60000.0|2013-04-01|\n",
            "|          6|          103|    Jill Wong| 32|Female|52000.0|2018-07-01|\n",
            "|          7|          101|James Johnson| 42|  Male|70000.0|2012-03-15|\n",
            "|          8|          102|     Kate Kim| 29|Female|51000.0|2019-10-01|\n",
            "|          9|          103|      Tom Tan| 33|  Male|58000.0|2016-06-01|\n",
            "|         10|          104|     Lisa Lee| 27|Female|47000.0|2018-08-01|\n",
            "|         11|          104|   David Park| 38|  Male|65000.0|2015-11-01|\n",
            "|         12|          105|   Susan Chen| 31|Female|54000.0|2017-02-15|\n",
            "|         13|          106|    Brian Kim| 45|  Male|75000.0|2011-07-01|\n",
            "|         14|          107|    Emily Lee| 26|Female|46000.0|2019-01-01|\n",
            "|         15|          106|  Michael Lee| 37|  Male|63000.0|2014-09-30|\n",
            "|         16|          107|  Kelly Zhang| 30|Female|49000.0|2018-04-01|\n",
            "|         17|          105|  George Wang| 34|  Male|57000.0|2016-03-15|\n",
            "|         18|          104|    Nancy Liu| 29|Female|50000.0|2017-06-01|\n",
            "|         19|          103|  Steven Chen| 36|  Male|62000.0|2015-08-01|\n",
            "|         20|          102|    Grace Kim| 32|Female|53000.0|2018-11-01|\n",
            "+-----------+-------------+-------------+---+------+-------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_schema.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "28a532d8-001d-4f3d-83a5-e96788c44128",
      "metadata": {
        "id": "28a532d8-001d-4f3d-83a5-e96788c44128"
      },
      "outputs": [],
      "source": [
        "# Handle BAD records - PERMISSIVE (Default mode)\n",
        "\n",
        "_schema = \"employee_id int, department_id int, name string, age int, gender string, salary double, hire_date date, bad_record string\"\n",
        "\n",
        "df_p = spark.read.format(\"csv\").schema(_schema).option(\"columnNameOfCorruptRecord\", \"bad_record\").option(\"header\", True).load(\"data/input/emp_new.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "fde49eee-2fa5-4e59-bc70-2b9fbc015347",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fde49eee-2fa5-4e59-bc70-2b9fbc015347",
        "outputId": "51cb93e8-17b8-4636-ca19-07a37c2c8892"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- employee_id: integer (nullable = true)\n",
            " |-- department_id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: double (nullable = true)\n",
            " |-- hire_date: date (nullable = true)\n",
            " |-- bad_record: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_p.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "683c850a-a071-42af-855f-5d82f349b8e5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "683c850a-a071-42af-855f-5d82f349b8e5",
        "outputId": "f634ab68-dca7-4b25-9fce-a0cbb6e9f4bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-------------+---+------+-------+----------+--------------------+\n",
            "|employee_id|department_id|         name|age|gender| salary| hire_date|          bad_record|\n",
            "+-----------+-------------+-------------+---+------+-------+----------+--------------------+\n",
            "|          1|          101|     John Doe| 30|  Male|50000.0|2015-01-01|                NULL|\n",
            "|          2|          101|   Jane Smith| 25|Female|45000.0|2016-02-15|                NULL|\n",
            "|          3|          102|    Bob Brown| 35|  Male|55000.0|2014-05-01|                NULL|\n",
            "|          4|          102|    Alice Lee| 28|Female|48000.0|2017-09-30|                NULL|\n",
            "|          5|          103|    Jack Chan| 40|  Male|60000.0|2013-04-01|                NULL|\n",
            "|          6|          103|    Jill Wong| 32|Female|52000.0|2018-07-01|                NULL|\n",
            "|          7|          101|James Johnson| 42|  Male|   NULL|2012-03-15|007,101,James Joh...|\n",
            "|          8|          102|     Kate Kim| 29|Female|51000.0|2019-10-01|                NULL|\n",
            "|          9|          103|      Tom Tan| 33|  Male|58000.0|2016-06-01|                NULL|\n",
            "|         10|          104|     Lisa Lee| 27|Female|47000.0|2018-08-01|                NULL|\n",
            "|         11|          104|   David Park| 38|  Male|65000.0|      NULL|011,104,David Par...|\n",
            "|         12|          105|   Susan Chen| 31|Female|54000.0|2017-02-15|                NULL|\n",
            "|         13|          106|    Brian Kim| 45|  Male|75000.0|2011-07-01|                NULL|\n",
            "|         14|          107|    Emily Lee| 26|Female|46000.0|2019-01-01|                NULL|\n",
            "|         15|          106|  Michael Lee| 37|  Male|63000.0|2014-09-30|                NULL|\n",
            "|         16|          107|  Kelly Zhang| 30|Female|49000.0|2018-04-01|                NULL|\n",
            "|         17|          105|  George Wang| 34|  Male|57000.0|2016-03-15|                NULL|\n",
            "|         18|          104|    Nancy Liu| 29|Female|50000.0|2017-06-01|                NULL|\n",
            "|         19|          103|  Steven Chen| 36|  Male|62000.0|2015-08-01|                NULL|\n",
            "|         20|          102|    Grace Kim| 32|Female|53000.0|2018-11-01|                NULL|\n",
            "+-----------+-------------+-------------+---+------+-------+----------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_p.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "ddcfdf1c-82ff-4776-a273-c3cc95079020",
      "metadata": {
        "id": "ddcfdf1c-82ff-4776-a273-c3cc95079020"
      },
      "outputs": [],
      "source": [
        "# Handle BAD records - DROPMALFORMED\n",
        "_schema = \"employee_id int, department_id int, name string, age int, gender string, salary double, hire_date date\"\n",
        "\n",
        "df_m = spark.read.format(\"csv\").option(\"header\", True).option(\"mode\", \"DROPMALFORMED\").schema(_schema).load(\"data/input/emp_new.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "e6464a80-472e-444e-b940-9aaf5540d149",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6464a80-472e-444e-b940-9aaf5540d149",
        "outputId": "e9d26454-b362-4ff6-91ce-13f582616e60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- employee_id: integer (nullable = true)\n",
            " |-- department_id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: double (nullable = true)\n",
            " |-- hire_date: date (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_m.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "a5e20a51-62e6-4f4a-a63b-3c14a0cd6265",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5e20a51-62e6-4f4a-a63b-3c14a0cd6265",
        "outputId": "901da5e9-affc-4439-bf32-88307b29e8bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-----------+---+------+-------+----------+\n",
            "|employee_id|department_id|       name|age|gender| salary| hire_date|\n",
            "+-----------+-------------+-----------+---+------+-------+----------+\n",
            "|          1|          101|   John Doe| 30|  Male|50000.0|2015-01-01|\n",
            "|          2|          101| Jane Smith| 25|Female|45000.0|2016-02-15|\n",
            "|          3|          102|  Bob Brown| 35|  Male|55000.0|2014-05-01|\n",
            "|          4|          102|  Alice Lee| 28|Female|48000.0|2017-09-30|\n",
            "|          5|          103|  Jack Chan| 40|  Male|60000.0|2013-04-01|\n",
            "|          6|          103|  Jill Wong| 32|Female|52000.0|2018-07-01|\n",
            "|          8|          102|   Kate Kim| 29|Female|51000.0|2019-10-01|\n",
            "|          9|          103|    Tom Tan| 33|  Male|58000.0|2016-06-01|\n",
            "|         10|          104|   Lisa Lee| 27|Female|47000.0|2018-08-01|\n",
            "|         12|          105| Susan Chen| 31|Female|54000.0|2017-02-15|\n",
            "|         13|          106|  Brian Kim| 45|  Male|75000.0|2011-07-01|\n",
            "|         14|          107|  Emily Lee| 26|Female|46000.0|2019-01-01|\n",
            "|         15|          106|Michael Lee| 37|  Male|63000.0|2014-09-30|\n",
            "|         16|          107|Kelly Zhang| 30|Female|49000.0|2018-04-01|\n",
            "|         17|          105|George Wang| 34|  Male|57000.0|2016-03-15|\n",
            "|         18|          104|  Nancy Liu| 29|Female|50000.0|2017-06-01|\n",
            "|         19|          103|Steven Chen| 36|  Male|62000.0|2015-08-01|\n",
            "|         20|          102|  Grace Kim| 32|Female|53000.0|2018-11-01|\n",
            "+-----------+-------------+-----------+---+------+-------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_m.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "6c792ff9-8ac6-44ff-b376-461d9c37cc16",
      "metadata": {
        "id": "6c792ff9-8ac6-44ff-b376-461d9c37cc16"
      },
      "outputs": [],
      "source": [
        "# Handle BAD records - FAILFAST\n",
        "\n",
        "_schema = \"employee_id int, department_id int, name string, age int, gender string, salary double, hire_date date\"\n",
        "\n",
        "df_m = spark.read.format(\"csv\").option(\"header\", True).option(\"mode\", \"FAILFAST\").schema(_schema).load(\"data/input/emp_new.csv\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "0f571cd7-3338-4d29-91d8-278c89f6091f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f571cd7-3338-4d29-91d8-278c89f6091f",
        "outputId": "9fd8c602-06b3-459a-9d5f-8d0b8582bd45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- employee_id: integer (nullable = true)\n",
            " |-- department_id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: double (nullable = true)\n",
            " |-- hire_date: date (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_m.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "8ba789ae-ebe4-45d4-9fac-8e6b2b4840cd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8ba789ae-ebe4-45d4-9fac-8e6b2b4840cd",
        "outputId": "eff820cf-ccad-499f-8ab1-943111c31f16"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o129.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 13.0 failed 1 times, most recent failure: Lost task 0.0 in stage 13.0 (TID 13) (50949f410618 executor driver): org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION] Malformed records are detected in record parsing: [7,101,James Johnson,42,Male,null,15414].\nParse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'. \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1610)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:79)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:456)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"Low\"\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:365)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:307)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:452)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n\t... 26 more\nCaused by: java.lang.NumberFormatException: For input string: \"Low\"\n\tat java.base/jdk.internal.math.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2054)\n\tat java.base/jdk.internal.math.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n\tat java.base/java.lang.Double.parseDouble(Double.java:543)\n\tat scala.collection.immutable.StringLike.toDouble(StringLike.scala:327)\n\tat scala.collection.immutable.StringLike.toDouble$(StringLike.scala:327)\n\tat scala.collection.immutable.StringOps.toDouble(StringOps.scala:33)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$12(UnivocityParser.scala:207)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:291)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$11(UnivocityParser.scala:203)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:346)\n\t... 29 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION] Malformed records are detected in record parsing: [7,101,James Johnson,42,Male,null,15414].\nParse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'. \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1610)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:79)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:456)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"Low\"\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:365)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:307)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:452)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n\t... 26 more\nCaused by: java.lang.NumberFormatException: For input string: \"Low\"\n\tat java.base/jdk.internal.math.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2054)\n\tat java.base/jdk.internal.math.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n\tat java.base/java.lang.Double.parseDouble(Double.java:543)\n\tat scala.collection.immutable.StringLike.toDouble(StringLike.scala:327)\n\tat scala.collection.immutable.StringLike.toDouble$(StringLike.scala:327)\n\tat scala.collection.immutable.StringOps.toDouble(StringOps.scala:33)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$12(UnivocityParser.scala:207)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:291)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$11(UnivocityParser.scala:203)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:346)\n\t... 29 more\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-780568131.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_m\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    943\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mBob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \"\"\"\n\u001b[0;32m--> 945\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m     def _show_string(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    964\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o129.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 13.0 failed 1 times, most recent failure: Lost task 0.0 in stage 13.0 (TID 13) (50949f410618 executor driver): org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION] Malformed records are detected in record parsing: [7,101,James Johnson,42,Male,null,15414].\nParse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'. \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1610)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:79)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:456)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"Low\"\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:365)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:307)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:452)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n\t... 26 more\nCaused by: java.lang.NumberFormatException: For input string: \"Low\"\n\tat java.base/jdk.internal.math.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2054)\n\tat java.base/jdk.internal.math.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n\tat java.base/java.lang.Double.parseDouble(Double.java:543)\n\tat scala.collection.immutable.StringLike.toDouble(StringLike.scala:327)\n\tat scala.collection.immutable.StringLike.toDouble$(StringLike.scala:327)\n\tat scala.collection.immutable.StringOps.toDouble(StringOps.scala:33)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$12(UnivocityParser.scala:207)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:291)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$11(UnivocityParser.scala:203)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:346)\n\t... 29 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION] Malformed records are detected in record parsing: [7,101,James Johnson,42,Male,null,15414].\nParse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'. \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1610)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:79)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:456)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"Low\"\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:365)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:307)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:452)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n\t... 26 more\nCaused by: java.lang.NumberFormatException: For input string: \"Low\"\n\tat java.base/jdk.internal.math.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2054)\n\tat java.base/jdk.internal.math.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n\tat java.base/java.lang.Double.parseDouble(Double.java:543)\n\tat scala.collection.immutable.StringLike.toDouble(StringLike.scala:327)\n\tat scala.collection.immutable.StringLike.toDouble$(StringLike.scala:327)\n\tat scala.collection.immutable.StringOps.toDouble(StringOps.scala:33)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$12(UnivocityParser.scala:207)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:291)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$11(UnivocityParser.scala:203)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:346)\n\t... 29 more\n"
          ]
        }
      ],
      "source": [
        "df_m.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "e1d5c62f-c5e7-403b-bca9-7658f6da8932",
      "metadata": {
        "id": "e1d5c62f-c5e7-403b-bca9-7658f6da8932"
      },
      "outputs": [],
      "source": [
        "# BONUS TIP\n",
        "# Multiple options\n",
        "\n",
        "_options = {\n",
        "    \"header\" : \"true\",\n",
        "    \"inferSchema\" : \"true\",\n",
        "    \"mode\" : \"PERMISSIVE\"\n",
        "}\n",
        "\n",
        "df = (spark.read.format(\"csv\").options(**_options).load(\"data/input/emp.csv\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "51266d28-6ab3-41b2-a5f1-e5b9599dbfa0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51266d28-6ab3-41b2-a5f1-e5b9599dbfa0",
        "outputId": "4b426598-34fb-4fc8-efbd-b9abfc6569ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-------------+---+------+------+----------+\n",
            "|employee_id|department_id|         name|age|gender|salary| hire_date|\n",
            "+-----------+-------------+-------------+---+------+------+----------+\n",
            "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01|\n",
            "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15|\n",
            "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|\n",
            "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30|\n",
            "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|\n",
            "|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01|\n",
            "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15|\n",
            "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01|\n",
            "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|\n",
            "|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n",
            "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01|\n",
            "|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15|\n",
            "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|\n",
            "|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01|\n",
            "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|\n",
            "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|\n",
            "|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15|\n",
            "|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|\n",
            "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|\n",
            "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01|\n",
            "+-----------+-------------+-------------+---+------+------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "607c1ce3-0483-479e-aeba-9a43109d8aa1",
      "metadata": {
        "id": "607c1ce3-0483-479e-aeba-9a43109d8aa1"
      },
      "outputs": [],
      "source": [
        "spark.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "afbd61a0-9426-4591-93d4-e0c5f398c2c9",
      "metadata": {
        "id": "afbd61a0-9426-4591-93d4-e0c5f398c2c9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}